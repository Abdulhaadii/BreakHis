{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LHnYxLNAMPU2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "AWO6daZqNMSE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"Label smoothing cross entropy loss.\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        log_prob = F.log_softmax(input, dim=-1)\n",
        "        weight = input.new_ones(input.size()) * self.smoothing / (input.size(-1) - 1.)\n",
        "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
        "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "zNilDKJxNO4J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping utility.\"\"\"\n",
        "\n",
        "    def __init__(self, patience: int = 10, min_delta: float = 0.001,\n",
        "                 mode: str = 'min'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score: float):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "        elif self.mode == 'min':\n",
        "            if val_score < self.best_score - self.min_delta:\n",
        "                self.best_score = val_score\n",
        "                self.counter = 0\n",
        "            else:\n",
        "                self.counter += 1\n",
        "        else:  # mode == 'max'\n",
        "            if val_score > self.best_score + self.min_delta:\n",
        "                self.best_score = val_score\n",
        "                self.counter = 0\n",
        "            else:\n",
        "                self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n"
      ],
      "metadata": {
        "id": "EfkBzNRCNVL_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCalculator:\n",
        "    \"\"\"Utility class for calculating classification metrics.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray,\n",
        "                         y_pred_proba: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calculate all classification metrics.\"\"\"\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='binary')\n",
        "        recall = recall_score(y_true, y_pred, average='binary')\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "        f1 = f1_score(y_true, y_pred, average='binary')\n",
        "        auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'f1': f1,\n",
        "            'auc_roc': auc_roc,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrix(cm: np.ndarray, title: str = \"Confusion Matrix\"):\n",
        "        \"\"\"Plot confusion matrix heatmap.\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Benign', 'Malignant'],\n",
        "                   yticklabels=['Benign', 'Malignant'])\n",
        "        plt.title(title)\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "GHtPtNJoNWnx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New MobileNetV2-based classifier\n",
        "class MobileNetV2Classifier(nn.Module):\n",
        "    \"\"\"MobileNetV2-based tumor classifier with improved architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 2, dropout_rate: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained MobileNetV2\n",
        "        self.backbone = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
        "\n",
        "        # Get the number of features from the last layer\n",
        "        in_features = self.backbone.classifier[1].in_features  # 1280\n",
        "\n",
        "        # Replace the final classifier with a more sophisticated head\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate * 0.5),  # Reduced dropout for final layer\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize classifier weights\n",
        "        self._init_classifier_weights()\n",
        "\n",
        "    def _init_classifier_weights(self):\n",
        "        \"\"\"Initialize classifier weights with Xavier initialization.\"\"\"\n",
        "        for m in self.backbone.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        \"\"\"Freeze backbone parameters except final classifier.\"\"\"\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if 'classifier' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze_last_layers(self, num_blocks: int = 3):\n",
        "        \"\"\"Unfreeze last few inverted residual blocks of MobileNetV2.\"\"\"\n",
        "        # MobileNetV2 has features organized as Sequential modules\n",
        "        # Unfreeze last num_blocks inverted residual blocks\n",
        "        total_blocks = len(self.backbone.features)\n",
        "        unfreeze_from = max(0, total_blocks - num_blocks)\n",
        "\n",
        "        for i, layer in enumerate(self.backbone.features):\n",
        "            if i >= unfreeze_from:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True"
      ],
      "metadata": {
        "id": "4JZG3SZqNZA5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified data transforms for better accuracy (320x320 input)\n",
        "def get_improved_transforms():\n",
        "    \"\"\"Get improved data transforms for higher accuracy.\"\"\"\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((360, 360)),  # Larger resize for better crop diversity\n",
        "        transforms.RandomResizedCrop(320, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=20),  # Increased rotation\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(brightness=0.3, contrast=0.3,\n",
        "                                 saturation=0.3, hue=0.1)\n",
        "        ], p=0.8),\n",
        "        transforms.RandomApply([\n",
        "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "        ], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "        # Random erasing for better generalization\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n",
        "    ])\n",
        "\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize((350, 350)),\n",
        "        transforms.CenterCrop(320),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transforms, val_transforms\n"
      ],
      "metadata": {
        "id": "8zXauW9-NlWc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class BreakHisDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "3j9Wi4tkTRLB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BreakHisTrainer:\n",
        "    def __init__(self, data_dir: str, batch_size=24, num_workers=4, device='cuda'):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.results: Dict[str, Dict] = {}\n",
        "        self.train_transforms, self.val_transforms = get_improved_transforms()\n",
        "\n",
        "    def parse_dataset(self) -> Dict[str, List]:\n",
        "        print(\"Parsing BreakHis dataset...\")\n",
        "        possible_paths = [\n",
        "            os.path.join(self.data_dir, 'BreaKHis_v1/BreaKHis_v1/histology_slides/breast/**/*.png'),\n",
        "            os.path.join(self.data_dir, 'BreaKHis_v1/histology_slides/breast/**/*.png'),\n",
        "            os.path.join(self.data_dir, 'histology_slides/breast/**/*.png'),\n",
        "            os.path.join(self.data_dir, '**/*.png'),\n",
        "        ]\n",
        "        breast_img_paths = []\n",
        "        for pattern in possible_paths:\n",
        "            breast_img_paths = glob.glob(pattern, recursive=True)\n",
        "            if breast_img_paths:\n",
        "                print(f\"Found images using pattern: {pattern}\")\n",
        "                break\n",
        "        if not breast_img_paths:\n",
        "            raise FileNotFoundError(f\"No PNG files found in {self.data_dir}. Please check the dataset path.\")\n",
        "        print(f\"Found {len(breast_img_paths)} image files\")\n",
        "\n",
        "        data_info = {'image_paths': [], 'labels': [], 'magnifications': [], 'patient_ids': []}\n",
        "        for img_path in breast_img_paths:\n",
        "            try:\n",
        "                low = img_path.lower()\n",
        "                if 'benign' in low:\n",
        "                    label = 0\n",
        "                elif 'malignant' in low:\n",
        "                    label = 1\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                filename = os.path.basename(img_path)\n",
        "                if '-40-' in filename:\n",
        "                    mag = '40X'\n",
        "                elif '-100-' in filename:\n",
        "                    mag = '100X'\n",
        "                elif '-200-' in filename:\n",
        "                    mag = '200X'\n",
        "                elif '-400-' in filename:\n",
        "                    mag = '400X'\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                parts = filename.split('-')\n",
        "                patient_id = '-'.join(parts[:3]) if len(parts) >= 3 else filename.split('.')[0]\n",
        "\n",
        "                data_info['image_paths'].append(img_path)\n",
        "                data_info['labels'].append(label)\n",
        "                data_info['magnifications'].append(mag)\n",
        "                data_info['patient_ids'].append(patient_id)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Parsed {len(data_info['image_paths'])} images\")\n",
        "        print(f\"Benign: {sum([l==0 for l in data_info['labels']])}, Malignant: {sum([l==1 for l in data_info['labels']])}\")\n",
        "        print(f\"Magnifications: {set(data_info['magnifications'])}\")\n",
        "        print(f\"Unique patients: {len(set(data_info['patient_ids']))}\")\n",
        "\n",
        "        return data_info\n",
        "\n",
        "    def create_patient_splits(self, data_info: Dict, test_size=0.2, val_size=0.2) -> Dict[str, Dict]:\n",
        "        df = pd.DataFrame(data_info)\n",
        "        patient_labels = df.groupby('patient_ids')['labels'].first().reset_index()\n",
        "\n",
        "        train_val_patients, test_patients = train_test_split(\n",
        "            patient_labels['patient_ids'], test_size=test_size,\n",
        "            stratify=patient_labels['labels'], random_state=42)\n",
        "        train_patients, val_patients = train_test_split(\n",
        "            train_val_patients,\n",
        "            test_size=val_size/(1-test_size),\n",
        "            stratify=patient_labels[patient_labels['patient_ids'].isin(train_val_patients)]['labels'],\n",
        "            random_state=42)\n",
        "\n",
        "        splits = {}\n",
        "        for mag in ['40X','100X','200X','400X','All']:\n",
        "            mask = df['magnifications'] == mag if mag != 'All' else pd.Series([True]*len(df))\n",
        "            mag_df = df[mask]\n",
        "            splits[mag] = {\n",
        "                'train': {\n",
        "                    'image_paths': mag_df[mag_df['patient_ids'].isin(train_patients)]['image_paths'].tolist(),\n",
        "                    'labels': mag_df[mag_df['patient_ids'].isin(train_patients)]['labels'].tolist(),\n",
        "                },\n",
        "                'val': {\n",
        "                    'image_paths': mag_df[mag_df['patient_ids'].isin(val_patients)]['image_paths'].tolist(),\n",
        "                    'labels': mag_df[mag_df['patient_ids'].isin(val_patients)]['labels'].tolist(),\n",
        "                },\n",
        "                'test': {\n",
        "                    'image_paths': mag_df[mag_df['patient_ids'].isin(test_patients)]['image_paths'].tolist(),\n",
        "                    'labels': mag_df[mag_df['patient_ids'].isin(test_patients)]['labels'].tolist(),\n",
        "                }\n",
        "            }\n",
        "            print(f\"{mag}: train={len(splits[mag]['train']['labels'])}, \"\n",
        "                  f\"val={len(splits[mag]['val']['labels'])}, test={len(splits[mag]['test']['labels'])}\")\n",
        "        return splits\n",
        "\n",
        "    def create_data_loaders(self, splits: Dict, magnification: str) -> Dict[str, DataLoader]:\n",
        "        sd = splits[magnification]\n",
        "        train_ds = BreakHisDataset(sd['train']['image_paths'], sd['train']['labels'], self.train_transforms)\n",
        "        val_ds = BreakHisDataset(sd['val']['image_paths'], sd['val']['labels'], self.val_transforms)\n",
        "        test_ds = BreakHisDataset(sd['test']['image_paths'], sd['test']['labels'], self.val_transforms)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "        test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "        return {\n",
        "            'train': train_loader,\n",
        "            'val': val_loader,\n",
        "            'test': test_loader\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, model, loader, criterion, optimizer) -> float:\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for images, labs in tqdm(loader, desc=\"Training\"):\n",
        "            images, labs = images.to(self.device), labs.to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total += loss.item()\n",
        "        return total / len(loader)\n",
        "\n",
        "    def validate_epoch(self, model, loader, criterion) -> Tuple[float, float]:\n",
        "        model.eval()\n",
        "        total = 0.0\n",
        "        all_p, all_l, all_proba = [], [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labs in tqdm(loader, desc=\"Validation\"):\n",
        "                images, labs = images.to(self.device), labs.to(self.device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labs)\n",
        "                total += loss.item()\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_p += preds.cpu().tolist()\n",
        "                all_l += labs.cpu().tolist()\n",
        "                all_proba += probs[:, 1].cpu().tolist()\n",
        "        return total / len(loader), roc_auc_score(all_l, all_proba)\n",
        "\n",
        "    train_magnification = train_magnification_mobilenet\n",
        "    test_model_with_tta = test_model_with_tta\n",
        "\n",
        "    def run_all_experiments(self, data_dir: str) -> pd.DataFrame:\n",
        "        print(\"Starting MobileNetV2 experiments…\")\n",
        "        data_info = self.parse_dataset()\n",
        "        splits = self.create_patient_splits(data_info)\n",
        "        results = []\n",
        "        for mag in ['40X','100X','200X','400X','All']:\n",
        "            print(f\"\\n== Magnification: {mag}\")\n",
        "            m = self.train_magnification(splits, mag)\n",
        "            results.append({\n",
        "                'Magnification': mag,\n",
        "                'accuracy': m['accuracy'],\n",
        "                'precision': m['precision'],\n",
        "                'recall': m['recall'],\n",
        "                'specificity': m['specificity'],\n",
        "                'f1': m['f1'],\n",
        "                'auc_roc': m['auc_roc']\n",
        "            })\n",
        "            self.results[mag] = {'metrics': results[-1], 'confusion_matrix': m['confusion_matrix']}\n",
        "        df = pd.DataFrame(results)\n",
        "        print(\"\\nFinal Results:\\n\", df.to_string(index=False))\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "v8hUhT0SNojQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified train_magnification method for MobileNetV2\n",
        "def train_magnification_mobilenet(self, splits: Dict, magnification: str,\n",
        "                                epochs: int = 120) -> Dict[str, float]:  # Increased epochs\n",
        "    \"\"\"Train MobileNetV2 model for a specific magnification with improvements.\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training on {magnification} magnification with MobileNetV2\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    data_loaders = self.create_data_loaders(splits, magnification)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MobileNetV2Classifier(num_classes=2, dropout_rate=0.4)  # Reduced dropout\n",
        "    model.to(self.device)\n",
        "\n",
        "    # Loss function with reduced label smoothing\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=0.05)\n",
        "\n",
        "    # Phase 1: Train classifier head with frozen backbone (longer training)\n",
        "    print(\"\\nPhase 1: Training classifier head (frozen backbone)\")\n",
        "    model.freeze_backbone()\n",
        "\n",
        "    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                    lr=0.002, weight_decay=2e-4)  # Higher LR and weight decay\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs//2, eta_min=1e-6)\n",
        "    early_stopping = EarlyStopping(patience=15, mode='max')  # More patience\n",
        "\n",
        "    best_auc = 0.0\n",
        "    best_model_state = None\n",
        "    phase1_epochs = int(epochs * 0.4)  # 40% of epochs for phase 1\n",
        "\n",
        "    for epoch in range(phase1_epochs):\n",
        "        # Training\n",
        "        train_loss = self.train_epoch(model, data_loaders['train'],\n",
        "                                    criterion, optimizer)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_auc = self.validate_epoch(model, data_loaders['val'],\n",
        "                                              criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "        early_stopping(-val_auc)  # Negative because we want to maximize AUC\n",
        "\n",
        "        print(f\"Epoch {epoch+1:3d}: Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model from Phase 1\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Phase 2: Fine-tune with unfrozen last layers\n",
        "    print(\"\\nPhase 2: Fine-tuning with unfrozen last layers\")\n",
        "    model.unfreeze_last_layers(num_blocks=4)  # Unfreeze more layers\n",
        "\n",
        "    # Lower learning rate for fine-tuning\n",
        "    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                    lr=0.0002, weight_decay=1e-4)  # Lower LR\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs-phase1_epochs, eta_min=1e-7)\n",
        "    early_stopping = EarlyStopping(patience=20, mode='max')  # More patience\n",
        "\n",
        "    for epoch in range(epochs - phase1_epochs):\n",
        "        # Training with gradient clipping for stability\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(data_loaders['train'], desc=\"Training\")):\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loss = total_loss / len(data_loaders['train'])\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_auc = self.validate_epoch(model, data_loaders['val'],\n",
        "                                              criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "        early_stopping(-val_auc)\n",
        "\n",
        "        print(f\"Epoch {phase1_epochs+epoch+1:3d}: Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping at epoch {phase1_epochs+epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model and test with TTA\n",
        "    model.load_state_dict(best_model_state)\n",
        "    test_metrics = self.test_model_with_tta(model, data_loaders['test'])\n",
        "\n",
        "    print(f\"\\nBest validation AUC: {best_auc:.4f}\")\n",
        "    print(f\"Test metrics with TTA:\")\n",
        "    for metric, value in test_metrics.items():\n",
        "        if metric != 'confusion_matrix':\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = test_metrics['confusion_matrix']\n",
        "    MetricsCalculator.plot_confusion_matrix(cm, f\"Confusion Matrix - {magnification} (MobileNetV2)\")\n",
        "\n",
        "    return test_metrics\n"
      ],
      "metadata": {
        "id": "4xcoBw0nNrMd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test-Time Augmentation for improved accuracy\n",
        "def test_model_with_tta(self, model: nn.Module, test_loader: DataLoader,\n",
        "                       tta_transforms: int = 5) -> Dict[str, float]:\n",
        "    \"\"\"Test the model with Test-Time Augmentation for improved accuracy.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    # TTA transforms\n",
        "    tta_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Testing with TTA\"):\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "            # Original prediction\n",
        "            outputs = model(images)\n",
        "            probs_sum = F.softmax(outputs, dim=1)\n",
        "\n",
        "            # TTA predictions\n",
        "            for _ in range(tta_transforms):\n",
        "                # Apply random augmentation\n",
        "                aug_images = []\n",
        "                for img in images:\n",
        "                    # Convert to PIL, augment, convert back\n",
        "                    img_pil = transforms.ToPILImage()(img.cpu())\n",
        "                    img_aug = transforms.Compose([\n",
        "                        transforms.RandomHorizontalFlip(p=0.5),\n",
        "                        transforms.RandomRotation(degrees=5),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                           std=[0.229, 0.224, 0.225])\n",
        "                    ])(img_pil)\n",
        "                    aug_images.append(img_aug)\n",
        "\n",
        "                aug_batch = torch.stack(aug_images).to(self.device)\n",
        "                aug_outputs = model(aug_batch)\n",
        "                probs_sum += F.softmax(aug_outputs, dim=1)\n",
        "\n",
        "            # Average predictions\n",
        "            avg_probs = probs_sum / (tta_transforms + 1)\n",
        "            preds = avg_probs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(avg_probs[:, 1].cpu().numpy())\n",
        "\n",
        "    return MetricsCalculator.calculate_metrics(\n",
        "        np.array(all_labels),\n",
        "        np.array(all_preds),\n",
        "        np.array(all_probs)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ZEYEDz75NunI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the MobileNetV2 experiments.\"\"\"\n",
        "\n",
        "    # Download dataset using kagglehub\n",
        "    import kagglehub\n",
        "\n",
        "    # Download dataset\n",
        "    path = kagglehub.dataset_download(\"ambarish/breakhis\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "\n",
        "    # Configuration for MobileNetV2\n",
        "    DATA_DIR = path\n",
        "    BATCH_SIZE = 24  # Reduced for 320x320 images\n",
        "    NUM_WORKERS = 4\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(\"Using MobileNetV2 backbone with improvements\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = BreakHisTrainer(\n",
        "        data_dir=DATA_DIR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    # Replace the train_magnification method\n",
        "    trainer.train_magnification = lambda *args, **kwargs: train_magnification_mobilenet(trainer, *args, **kwargs)\n",
        "    trainer.test_model_with_tta = lambda *args, **kwargs: test_model_with_tta(trainer, *args, **kwargs)\n",
        "\n",
        "    # Run all experiments\n",
        "    results_df = trainer.run_all_experiments(DATA_DIR)\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_csv('breakhis_mobilenetv2_improved_results.csv', index=False)\n",
        "    print(\"\\nResults saved to 'breakhis_mobilenetv2_improved_results.csv'\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvqcAjD5Nxug",
        "outputId": "712d4ae8-6507-4e84-d7b6-ad9432a94167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/breakhis\n",
            "Using device: cuda\n",
            "Using MobileNetV2 backbone with improvements\n",
            "Starting MobileNetV2 experiments…\n",
            "Parsing BreakHis dataset...\n",
            "Found images using pattern: /kaggle/input/breakhis/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/**/*.png\n",
            "Found 7909 image files\n",
            "Parsed 7909 images\n",
            "Benign: 2480, Malignant: 5429\n",
            "Magnifications: {'400X', '100X', '200X', '40X'}\n",
            "Unique patients: 82\n",
            "40X: train=1172, val=382, test=441\n",
            "100X: train=1201, val=403, test=477\n",
            "200X: train=1188, val=382, test=443\n",
            "400X: train=1059, val=367, test=394\n",
            "All: train=4620, val=1534, test=1755\n",
            "\n",
            "== Magnification: 40X\n",
            "\n",
            "============================================================\n",
            "Training on 40X magnification with MobileNetV2\n",
            "============================================================\n",
            "\n",
            "Phase 1: Training classifier head (frozen backbone)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1: Train Loss: 0.7834, Val Loss: 0.8268, Val AUC: 0.7903, LR: 0.001999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2: Train Loss: 0.5332, Val Loss: 0.5554, Val AUC: 0.8114, LR: 0.001995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.64it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3: Train Loss: 0.4672, Val Loss: 0.5277, Val AUC: 0.8647, LR: 0.001988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.58it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4: Train Loss: 0.4296, Val Loss: 0.4502, Val AUC: 0.9024, LR: 0.001978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.64it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5: Train Loss: 0.4329, Val Loss: 0.4368, Val AUC: 0.9048, LR: 0.001966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.62it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   6: Train Loss: 0.4145, Val Loss: 0.4274, Val AUC: 0.9320, LR: 0.001951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   7: Train Loss: 0.4313, Val Loss: 0.4662, Val AUC: 0.9174, LR: 0.001934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.60it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   8: Train Loss: 0.3841, Val Loss: 0.4517, Val AUC: 0.9124, LR: 0.001914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   9: Train Loss: 0.3944, Val Loss: 0.5129, Val AUC: 0.9229, LR: 0.001891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10: Train Loss: 0.3821, Val Loss: 0.4405, Val AUC: 0.9260, LR: 0.001866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.62it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  11: Train Loss: 0.3828, Val Loss: 0.4455, Val AUC: 0.9200, LR: 0.001839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.62it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  12: Train Loss: 0.3779, Val Loss: 0.4442, Val AUC: 0.9275, LR: 0.001809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  13: Train Loss: 0.3790, Val Loss: 0.4539, Val AUC: 0.9141, LR: 0.001777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  14: Train Loss: 0.3816, Val Loss: 0.4382, Val AUC: 0.9106, LR: 0.001743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.63it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15: Train Loss: 0.3640, Val Loss: 0.4763, Val AUC: 0.8733, LR: 0.001707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.60it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  16: Train Loss: 0.3637, Val Loss: 0.4790, Val AUC: 0.9116, LR: 0.001669\n",
            "Early stopping at epoch 16\n",
            "\n",
            "Phase 2: Fine-tuning with unfrozen last layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  49: Train Loss: 0.3509, Val Loss: 0.4483, Val AUC: 0.9171, LR: 0.000200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  50: Train Loss: 0.3337, Val Loss: 0.4936, Val AUC: 0.9149, LR: 0.000200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  51: Train Loss: 0.3233, Val Loss: 0.4765, Val AUC: 0.9173, LR: 0.000199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.59it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  52: Train Loss: 0.3023, Val Loss: 0.4733, Val AUC: 0.9248, LR: 0.000198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  53: Train Loss: 0.2913, Val Loss: 0.4415, Val AUC: 0.9255, LR: 0.000198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  54: Train Loss: 0.2914, Val Loss: 0.4279, Val AUC: 0.9225, LR: 0.000197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  55: Train Loss: 0.2820, Val Loss: 0.4385, Val AUC: 0.9016, LR: 0.000195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.60it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  56: Train Loss: 0.2937, Val Loss: 0.4600, Val AUC: 0.9048, LR: 0.000194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  57: Train Loss: 0.2719, Val Loss: 0.4472, Val AUC: 0.9096, LR: 0.000192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  58: Train Loss: 0.2796, Val Loss: 0.4357, Val AUC: 0.9310, LR: 0.000191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.64it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  59: Train Loss: 0.2593, Val Loss: 0.4550, Val AUC: 0.9207, LR: 0.000189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  60: Train Loss: 0.2673, Val Loss: 0.4920, Val AUC: 0.9013, LR: 0.000187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  61: Train Loss: 0.2691, Val Loss: 0.4330, Val AUC: 0.9211, LR: 0.000184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  62: Train Loss: 0.2619, Val Loss: 0.4396, Val AUC: 0.9192, LR: 0.000182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.60it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  63: Train Loss: 0.2549, Val Loss: 0.4375, Val AUC: 0.9238, LR: 0.000179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.63it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  64: Train Loss: 0.2613, Val Loss: 0.4340, Val AUC: 0.9302, LR: 0.000177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  65: Train Loss: 0.2408, Val Loss: 0.4615, Val AUC: 0.9250, LR: 0.000174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  66: Train Loss: 0.2554, Val Loss: 0.5149, Val AUC: 0.9008, LR: 0.000171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:31<00:00,  1.57it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  67: Train Loss: 0.2518, Val Loss: 0.4802, Val AUC: 0.9160, LR: 0.000168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.63it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  68: Train Loss: 0.2445, Val Loss: 0.4739, Val AUC: 0.9126, LR: 0.000164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.64it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  69: Train Loss: 0.2383, Val Loss: 0.4727, Val AUC: 0.9045, LR: 0.000161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.64it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  70: Train Loss: 0.2427, Val Loss: 0.4676, Val AUC: 0.9181, LR: 0.000157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.61it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  71: Train Loss: 0.2518, Val Loss: 0.5015, Val AUC: 0.8688, LR: 0.000154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:31<00:00,  1.58it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  72: Train Loss: 0.2497, Val Loss: 0.5052, Val AUC: 0.8542, LR: 0.000150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.61it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  73: Train Loss: 0.2493, Val Loss: 0.5087, Val AUC: 0.8771, LR: 0.000146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  74: Train Loss: 0.2403, Val Loss: 0.4939, Val AUC: 0.8716, LR: 0.000142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  75: Train Loss: 0.2379, Val Loss: 0.5044, Val AUC: 0.8477, LR: 0.000138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.59it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  76: Train Loss: 0.2343, Val Loss: 0.5476, Val AUC: 0.8293, LR: 0.000134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  77: Train Loss: 0.2495, Val Loss: 0.5220, Val AUC: 0.8603, LR: 0.000130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  78: Train Loss: 0.2383, Val Loss: 0.5073, Val AUC: 0.8310, LR: 0.000126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  79: Train Loss: 0.2348, Val Loss: 0.5136, Val AUC: 0.8388, LR: 0.000122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.59it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  80: Train Loss: 0.2422, Val Loss: 0.5123, Val AUC: 0.8649, LR: 0.000117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  81: Train Loss: 0.2420, Val Loss: 0.5251, Val AUC: 0.8210, LR: 0.000113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  82: Train Loss: 0.2353, Val Loss: 0.4971, Val AUC: 0.8455, LR: 0.000109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  83: Train Loss: 0.2288, Val Loss: 0.5087, Val AUC: 0.8640, LR: 0.000104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  84: Train Loss: 0.2318, Val Loss: 0.5086, Val AUC: 0.8483, LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.65it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  85: Train Loss: 0.2459, Val Loss: 0.4972, Val AUC: 0.8473, LR: 0.000096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.66it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  86: Train Loss: 0.2364, Val Loss: 0.5116, Val AUC: 0.8654, LR: 0.000091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.61it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  87: Train Loss: 0.2272, Val Loss: 0.5126, Val AUC: 0.8577, LR: 0.000087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.63it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  88: Train Loss: 0.2344, Val Loss: 0.5017, Val AUC: 0.8640, LR: 0.000083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  89: Train Loss: 0.2325, Val Loss: 0.5011, Val AUC: 0.8713, LR: 0.000078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  90: Train Loss: 0.2361, Val Loss: 0.5166, Val AUC: 0.8542, LR: 0.000074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:30<00:00,  1.62it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  91: Train Loss: 0.2370, Val Loss: 0.5113, Val AUC: 0.8611, LR: 0.000070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  92: Train Loss: 0.2268, Val Loss: 0.4963, Val AUC: 0.8258, LR: 0.000066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  93: Train Loss: 0.2320, Val Loss: 0.5011, Val AUC: 0.8558, LR: 0.000062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [00:29<00:00,  1.68it/s]\n",
            "Validation: 100%|██████████| 16/16 [00:05<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  94: Train Loss: 0.2315, Val Loss: 0.5137, Val AUC: 0.8634, LR: 0.000058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 47/49 [00:30<00:01,  1.77it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yU_jaHVsOQhT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}